{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 3. Model Comparison : Comparaison de différentes architectures CNN pour MIAS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Introduction\n",
                "\n",
                "Ce notebook a pour objectif de comparer plusieurs architectures de réseaux de neurones convolutifs (CNN) pour la classification des mammographies du dataset MIAS.\n",
                "\n",
                "### Objectifs :\n",
                "- Implémenter un modèle baseline simple\n",
                "- Tester un modèle CNN optimisé\n",
                "- Expérimenter le transfert d’apprentissage (ResNet, etc.)\n",
                "- Évaluer et comparer les performances de chaque modèle (accuracy, recall, F1-score)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Importation des librairies\n",
                "\n",
                "Nous importons les librairies nécessaires pour la manipulation des données, la construction des modèles CNN et l’évaluation des performances."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Librairies importées avec succès.\n"
                    ]
                }
            ],
            "source": [
                "# ----- Import libraries PEP 8 -----\n",
                "# ----- Standard library -----\n",
                "import numpy as np # Pour les opérations mathématiques et les tableaux \n",
                "import pandas as pd # Pour manipuler et analyser les métadonnées\n",
                "import matplotlib.pyplot as plt # Pour les visualisations \n",
                "import tensorflow as tf # Pour le deep learning et la création de modèles\n",
                "from tensorflow.keras import layers, models, applications\n",
                "from sklearn.metrics import classification_report, confusion_matrix # Pour le rapport de classification\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import pickle\n",
                "\n",
                "print(\"Librairies importées avec succès.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Chargement des données\n",
                "\n",
                "Les données doivent être préparées comme dans le notebook précédent.  \n",
                "On charge ici les ensembles d’entraînement et de test (images et labels)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train shape : (264, 128, 128), Test shape : (66, 128, 128)\n",
                        "Train labels : ['ARCH' 'ASYM' 'CALC' 'CIRC' 'MISC' 'NORM' 'SPIC'], Test labels : ['ARCH' 'ASYM' 'CALC' 'CIRC' 'MISC' 'NORM' 'SPIC']\n"
                    ]
                }
            ],
            "source": [
                "X_train = np.load(\"../data/processed/X_train.npy\") # Charge les images d'entraînement depuis le fichier numpy\n",
                "X_test = np.load(\"../data/processed/X_test.npy\")   # Charge les images de test depuis le fichier numpy\n",
                "y_train = np.load(\"../data/processed/y_train.npy\") # Charge les labels d'entraînement depuis le fichier numpy\n",
                "y_test = np.load(\"../data/processed/y_test.npy\")   # Charge les labels de test depuis le fichier numpy\n",
                "\n",
                "print(f\"Train shape : {X_train.shape}, Test shape : {X_test.shape}\") # Affiche la forme des ensembles d'entraînement et de test\n",
                "print(f\"Train labels : {np.unique(y_train)}, Test labels : {np.unique(y_test)}\") # Affiche les classes présentes dans chaque ensemble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Labels encodés : [0 1 2 3 4 5 6]\n"
                    ]
                }
            ],
            "source": [
                "# Encodage des labels en entiers pour la classification multi-classe\n",
                "\n",
                "le = LabelEncoder()\n",
                "y_train = le.fit_transform(y_train) # Transforme les labels d'entraînement en entiers\n",
                "y_test = le.transform(y_test) # Transforme les labels de test en entiers\n",
                "print(f\"Labels encodés : {np.unique(y_train)}\") # Affiche les labels encodés"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Définition des architectures de modèles\n",
                "\n",
                "Nous allons définir trois architectures :\n",
                "- Un modèle baseline simple (CNN classique)\n",
                "- Un modèle CNN optimisé (plus de couches, dropout)\n",
                "- Un modèle utilisant le transfert d’apprentissage (ResNet50)\n",
                "\n",
                "Chaque modèle sera entraîné et évalué sur le même jeu de données pour une comparaison équitable."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Modèle baseline simple (CNN classique)\n",
                "\n",
                "Ce modèle sert de référence : il est composé de quelques couches de convolution et de pooling, suivi d’une couche dense pour la classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_simple_cnn(input_shape, num_classes): # input_shape : tuple qui définit la forme des images en entrée (ex : (128, 128, 1)), num_classes : entier, nombre de classes à prédire\n",
                "    model = tf.keras.Sequential([ # création d'un modèle séquentiel Keras, qui empile les couches les unes sur les autres\n",
                "        # --- Couche d'entrée ---\n",
                "        tf.keras.layers.Input(shape=input_shape), # # couche d'entrée qui définit la forme la forme attendue pour chaque image\n",
                "\n",
                "        # --- Couche intermédiaire ---\n",
                "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # Première couche de convolution : 32 : nombre de filtres (sorties), (3, 3) : taille du filtre (hauteur, largeur)\n",
                "        tf.keras.layers.MaxPooling2D(2, 2), # Couche de sous-échantillonnage : réduit la taille de moitié\n",
                "        tf.keras.layers.Flatten(), # Transforme la matrice 2D en un vecteur 1D (pour la couche dense)\n",
                "        tf.keras.layers.Dense(64, activation='relu'), # Couche dense avec 64 neurones et activation ReLU\n",
                "\n",
                "        # --- Couche de sortie ---\n",
                "        tf.keras.layers.Dense(num_classes, activation='softmax') # Couche de sortie avec un neurone par classe et activation softmax\n",
                "    ])\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Modèle CNN optimisé\n",
                "\n",
                "Ce modèle ajoute une couche de convolution supplémentaire, du dropout pour limiter l’overfitting, et une couche dense plus large."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_optimized_cnn(input_shape, num_classes): # input_shape : tuple (hauteur, largeur, canaux), num_classes : nombre de classes à prédire\n",
                "    model = tf.keras.Sequential([ # Création d'un modèle séquentiel Keras\n",
                "        # --- Couche d'entrée ---\n",
                "        tf.keras.layers.Input(shape=input_shape), # Couche d'entrée, définit la forme attendue des images\n",
                "\n",
                "        # --- Couche intermédiaire ---\n",
                "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # 1ère couche de convolution : 32 filtres, taille 3x3, activation ReLU\n",
                "        tf.keras.layers.MaxPooling2D(2, 2), # 1er pooling : réduit la taille de moitié\n",
                "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # 2ème couche de convolution : 64 filtres, taille 3x3, activation ReLU\n",
                "        tf.keras.layers.MaxPooling2D(2, 2), # 2ème pooling : réduit la taille de moitié\n",
                "        tf.keras.layers.Dropout(0.3), # Dropout : désactive 30% des neurones pour limiter l'overfitting\n",
                "        tf.keras.layers.Flatten(), # Transforme la matrice 2D en vecteur 1D\n",
                "        tf.keras.layers.Dense(128, activation='relu'), # Couche dense : 128 neurones, activation ReLU\n",
                "        tf.keras.layers.Dropout(0.3), # Dropout : désactive 30% des neurones\n",
                "\n",
                "        # --- Couche de sortie ---\n",
                "        tf.keras.layers.Dense(num_classes, activation='softmax') # Couche de sortie : un neurone par classe, activation softmax\n",
                "    ])\n",
                "    return model # Retourne le modèle construit"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Modèle avec transfert d’apprentissage (ResNet50)\n",
                "\n",
                "Ce modèle utilise l’architecture ResNet50, connue pour ses très bonnes performances en vision par ordinateur.  \n",
                "On adapte la sortie pour la classification MIAS et on ajoute une couche dense pour enrichir l’apprentissage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_resnet_model(input_shape, num_classes): # input_shape : tuple (hauteur, largeur, canaux), num_classes : nombre de classes à prédire\n",
                "    base_model = tf.keras.applications.ResNet50( # Charge le modèle ResNet50\n",
                "        weights=None, # Pas de poids pré-entraînés (mettre 'imagenet' pour utiliser les poids ImageNet)\n",
                "        include_top=False, # Ne garde pas la couche de classification finale de ResNet50\n",
                "        input_shape=input_shape # Définit la forme attendue des images\n",
                "    )\n",
                "    \n",
                "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output) # Applique un pooling global pour réduire la dimension\n",
                "    x = tf.keras.layers.Dense(128, activation='relu')(x) # Couche dense de 128 neurones avec activation ReLU\n",
                "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(x) # Couche de sortie : un neurone par classe, activation softmax\n",
                "    model = tf.keras.Model(inputs=base_model.input, outputs=output) # Crée le modèle final en reliant l'entrée de ResNet50 à la sortie personnalisée\n",
                "    return model # Retourne le modèle construit"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Entraînement et comparaison des modèles\n",
                "\n",
                "Ce bloc de code entraîne les trois modèles (baseline, optimisé, ResNet) et sauvegarde leurs historiques pour une analyse ultérieure."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Préparation des données pour les modèles CNN\n",
                "\n",
                "Avant d'entraîner les modèles, il est nécessaire d'ajouter une dimension canal aux images (pour Conv2D) et de définir la forme d'entrée et le nombre de classes. Cette étape garantit que les données sont compatibles avec les architectures Keras utilisées dans ce notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_cnn = X_train[..., np.newaxis] # Ajoute une dimension canal aux images d'entraînement (niveaux de gris -> (batch, height, width, 1))\n",
                "X_test_cnn = X_test[..., np.newaxis]   # Ajoute une dimension canal aux images de test\n",
                "input_shape = X_train_cnn.shape[1:]    # Récupère la forme des images (hauteur, largeur, canaux) pour l'entrée du modèle\n",
                "num_classes = len(np.unique(y_train))  # Calcule le nombre de classes à prédire"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Création et compilation du modèle baseline\n",
                "\n",
                "On construit ici le modèle CNN de base (architecture simple) et on le compile avec l'optimiseur Adam et la fonction de perte adaptée à la classification multi-classe. Cette étape prépare le modèle pour l'entraînement."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_baseline = build_simple_cnn(input_shape, num_classes) # Initialise le modèle CNN baseline avec la forme d'entrée et le nombre de classes\n",
                "model_baseline.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Compile le modèle avec Adam et la fonction de perte adaptée à la classification multi-classe"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Entraînement et sauvegarde du modèle baseline\n",
                "\n",
                "On entraîne ici le modèle baseline sur les données d'entraînement, puis on sauvegarde l'historique d'entraînement (accuracy, loss) dans un fichier .pkl et le modèle lui-même au format .keras pour une utilisation ultérieure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 253ms/step - accuracy: 0.3555 - loss: 3.6292 - val_accuracy: 0.6038 - val_loss: 1.6330\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 253ms/step - accuracy: 0.3555 - loss: 3.6292 - val_accuracy: 0.6038 - val_loss: 1.6330\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.6019 - loss: 1.5054 - val_accuracy: 0.6604 - val_loss: 1.1486\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.6019 - loss: 1.5054 - val_accuracy: 0.6604 - val_loss: 1.1486\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 0.6209 - loss: 1.2852 - val_accuracy: 0.6415 - val_loss: 1.2050\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 0.6209 - loss: 1.2852 - val_accuracy: 0.6415 - val_loss: 1.2050\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.6351 - loss: 1.2391 - val_accuracy: 0.6981 - val_loss: 1.0866\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.6351 - loss: 1.2391 - val_accuracy: 0.6981 - val_loss: 1.0866\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 0.6303 - loss: 1.2113 - val_accuracy: 0.6981 - val_loss: 1.1028\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 0.6303 - loss: 1.2113 - val_accuracy: 0.6981 - val_loss: 1.1028\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.6493 - loss: 1.1303 - val_accuracy: 0.6792 - val_loss: 1.1992\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.6493 - loss: 1.1303 - val_accuracy: 0.6792 - val_loss: 1.1992\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.6493 - loss: 1.0749 - val_accuracy: 0.6792 - val_loss: 1.1971\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.6493 - loss: 1.0749 - val_accuracy: 0.6792 - val_loss: 1.1971\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6588 - loss: 1.0076 - val_accuracy: 0.6792 - val_loss: 1.1511\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6588 - loss: 1.0076 - val_accuracy: 0.6792 - val_loss: 1.1511\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 0.6493 - loss: 0.9671 - val_accuracy: 0.6792 - val_loss: 1.2169\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 0.6493 - loss: 0.9671 - val_accuracy: 0.6792 - val_loss: 1.2169\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 0.6682 - loss: 0.9049 - val_accuracy: 0.6604 - val_loss: 1.1857\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 0.6682 - loss: 0.9049 - val_accuracy: 0.6604 - val_loss: 1.1857\n"
                    ]
                }
            ],
            "source": [
                "history_baseline = model_baseline.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2) # Entraîne le modèle baseline sur les données d'entraînement (10 époques, batch de 32, 20% validation)\n",
                "\n",
                "with open(\"../models/history_baseline.pkl\", \"wb\") as f: # Ouvre/crée le fichier pour sauvegarder l'historique d'entraînement\n",
                "    pickle.dump(history_baseline.history, f) # Sauvegarde le dictionnaire d'historique (accuracy, loss, etc.) dans le fichier .pkl\n",
                "model_baseline.save(\"../models/model_baseline.keras\") # Sauvegarde le modèle entraîné au format .keras pour une utilisation ultérieure"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4 Création et compilation du modèle CNN optimisé\n",
                "\n",
                "On construit ici le modèle CNN optimisé (architecture plus complexe avec plus de couches et du dropout) et on le compile avec l'optimiseur Adam et la fonction de perte adaptée à la classification multi-classe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_optimized = build_optimized_cnn(input_shape, num_classes) # Initialise le modèle CNN optimisé avec la forme d'entrée et le nombre de classes\n",
                "model_optimized.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Compile le modèle avec Adam et la fonction de perte adaptée à la classification multi-classe"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.5 Entraînement et sauvegarde du modèle optimisé\n",
                "\n",
                "On entraîne ici le modèle CNN optimisé sur les données d'entraînement, puis on sauvegarde l'historique d'entraînement (accuracy, loss) dans un fichier .pkl et le modèle lui-même au format .keras pour une utilisation ultérieure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 324ms/step - accuracy: 0.5261 - loss: 1.6027 - val_accuracy: 0.6792 - val_loss: 1.2367\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 324ms/step - accuracy: 0.5261 - loss: 1.6027 - val_accuracy: 0.6792 - val_loss: 1.2367\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 260ms/step - accuracy: 0.6161 - loss: 1.3487 - val_accuracy: 0.6792 - val_loss: 1.1865\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 260ms/step - accuracy: 0.6161 - loss: 1.3487 - val_accuracy: 0.6792 - val_loss: 1.1865\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 0.6161 - loss: 1.2457 - val_accuracy: 0.6981 - val_loss: 1.1952\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 0.6161 - loss: 1.2457 - val_accuracy: 0.6981 - val_loss: 1.1952\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - accuracy: 0.6256 - loss: 1.1791 - val_accuracy: 0.6981 - val_loss: 1.1348\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - accuracy: 0.6256 - loss: 1.1791 - val_accuracy: 0.6981 - val_loss: 1.1348\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - accuracy: 0.6351 - loss: 1.1246 - val_accuracy: 0.6981 - val_loss: 1.1323\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - accuracy: 0.6351 - loss: 1.1246 - val_accuracy: 0.6981 - val_loss: 1.1323\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 339ms/step - accuracy: 0.6493 - loss: 1.0030 - val_accuracy: 0.6981 - val_loss: 1.1286\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 339ms/step - accuracy: 0.6493 - loss: 1.0030 - val_accuracy: 0.6981 - val_loss: 1.1286\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 300ms/step - accuracy: 0.6493 - loss: 0.9474 - val_accuracy: 0.6792 - val_loss: 1.1879\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 300ms/step - accuracy: 0.6493 - loss: 0.9474 - val_accuracy: 0.6792 - val_loss: 1.1879\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 310ms/step - accuracy: 0.6967 - loss: 0.8723 - val_accuracy: 0.6792 - val_loss: 1.1946\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 310ms/step - accuracy: 0.6967 - loss: 0.8723 - val_accuracy: 0.6792 - val_loss: 1.1946\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - accuracy: 0.7014 - loss: 0.7756 - val_accuracy: 0.6226 - val_loss: 1.2171\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - accuracy: 0.7014 - loss: 0.7756 - val_accuracy: 0.6226 - val_loss: 1.2171\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281ms/step - accuracy: 0.8057 - loss: 0.5990 - val_accuracy: 0.6792 - val_loss: 1.3057\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281ms/step - accuracy: 0.8057 - loss: 0.5990 - val_accuracy: 0.6792 - val_loss: 1.3057\n"
                    ]
                }
            ],
            "source": [
                "history_optimized = model_optimized.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2) # Entraîne le modèle optimisé sur les données d'entraînement (10 époques, batch de 32, 20% validation)\n",
                "\n",
                "with open(\"../models/history_optimized.pkl\", \"wb\") as f: # Ouvre/crée le fichier pour sauvegarder l'historique d'entraînement\n",
                "    pickle.dump(history_optimized.history, f) # Sauvegarde le dictionnaire d'historique (accuracy, loss, etc.) dans le fichier .pkl\n",
                "model_optimized.save(\"../models/model_optimized.keras\") # Sauvegarde le modèle entraîné au format .keras pour une utilisation ultérieure"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.6 Création et compilation du modèle ResNet50\n",
                "\n",
                "On construit ici le modèle ResNet50 (transfert d'apprentissage) et on le compile avec l'optimiseur Adam et la fonction de perte adaptée à la classification multi-classe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_resnet = build_resnet_model(input_shape, num_classes) # Initialise le modèle ResNet50 avec la forme d'entrée et le nombre de classes\n",
                "model_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Compile le modèle avec Adam et la fonction de perte adaptée à la classification multi-classe"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.7 Entraînement et sauvegarde du modèle ResNet50\n",
                "\n",
                "On entraîne ici le modèle ResNet50 sur les données d'entraînement, puis on sauvegarde l'historique d'entraînement (accuracy, loss) dans un fichier .pkl et le modèle lui-même au format .keras pour une utilisation ultérieure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.4028 - loss: 3.4579 - val_accuracy: 0.6792 - val_loss: 1.8104\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.4028 - loss: 3.4579 - val_accuracy: 0.6792 - val_loss: 1.8104\n",
                        "Epoch 2/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.6914 - val_accuracy: 0.6792 - val_loss: 1.8385\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.6914 - val_accuracy: 0.6792 - val_loss: 1.8385\n",
                        "Epoch 3/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - accuracy: 0.5877 - loss: 1.5967 - val_accuracy: 0.6792 - val_loss: 1.7350\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - accuracy: 0.5877 - loss: 1.5967 - val_accuracy: 0.6792 - val_loss: 1.7350\n",
                        "Epoch 4/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.6019 - loss: 1.4872 - val_accuracy: 0.6792 - val_loss: 1.7197\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.6019 - loss: 1.4872 - val_accuracy: 0.6792 - val_loss: 1.7197\n",
                        "Epoch 5/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.3388 - val_accuracy: 0.6792 - val_loss: 1.6199\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.3388 - val_accuracy: 0.6792 - val_loss: 1.6199\n",
                        "Epoch 6/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6161 - loss: 1.2193 - val_accuracy: 0.6792 - val_loss: 1.6160\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6161 - loss: 1.2193 - val_accuracy: 0.6792 - val_loss: 1.6160\n",
                        "Epoch 7/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6303 - loss: 1.2404 - val_accuracy: 0.6792 - val_loss: 1.5635\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6303 - loss: 1.2404 - val_accuracy: 0.6792 - val_loss: 1.5635\n",
                        "Epoch 8/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.1888 - val_accuracy: 0.6792 - val_loss: 1.6392\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6114 - loss: 1.1888 - val_accuracy: 0.6792 - val_loss: 1.6392\n",
                        "Epoch 9/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4s/step - accuracy: 0.6682 - loss: 1.0694 - val_accuracy: 0.6792 - val_loss: 1.3963\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4s/step - accuracy: 0.6682 - loss: 1.0694 - val_accuracy: 0.6792 - val_loss: 1.3963\n",
                        "Epoch 10/10\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - accuracy: 0.6066 - loss: 1.0246 - val_accuracy: 0.6792 - val_loss: 1.3237\n",
                        "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - accuracy: 0.6066 - loss: 1.0246 - val_accuracy: 0.6792 - val_loss: 1.3237\n"
                    ]
                }
            ],
            "source": [
                "history_resnet = model_resnet.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2) # Entraîne le modèle ResNet50 sur les données d'entraînement (10 époques, batch de 32, 20% validation)\n",
                "\n",
                "with open(\"../models/history_resnet.pkl\", \"wb\") as f: # Ouvre/crée le fichier pour sauvegarder l'historique d'entraînement\n",
                "    pickle.dump(history_resnet.history, f) # Sauvegarde le dictionnaire d'historique (accuracy, loss, etc.) dans le fichier .pkl\n",
                "model_resnet.save(\"../models/model_resnet.keras\") # Sauvegarde le modèle entraîné au format .keras pour une utilisation ultérieure"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Conclusion\n",
                "\n",
                "Dans ce notebook, nous avons comparé plusieurs architectures CNN pour la classification des mammographies MIAS :  \n",
                "- Un modèle baseline simple  \n",
                "- Un modèle CNN optimisé  \n",
                "- Un modèle utilisant le transfert d'apprentissage (ResNet50)\n",
                "\n",
                "Chaque modèle a été entraîné et évalué sur le même jeu de données.  \n",
                "Les modèles et leurs historiques d'entraînement ont été sauvegardés pour une analyse ultérieure.  \n",
                "Les résultats bruts (accuracy, loss) sont prêts à être analysés plus en détail dans le prochain notebook.\n",
                "\n",
                "Le notebook suivant présentera une analyse approfondie des performances, des matrices de confusion et des rapports de classification pour interpréter les résultats et identifier le meilleur modèle pour cette tâche de classification."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
